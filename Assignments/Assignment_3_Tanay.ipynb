{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a list words = ['is', 'it', 'good', '?']. a) Use a series of assignment statements (e.g. words[1] = words[2]) and a temporary variable tmp to transform this list into the list ['it', 'is', 'good', '!']. b) Now do the same transformation using tuple assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Using tmp varible\n",
    "\n",
    "# Transform to ['it', 'is', 'good', '?']\n",
    "\n",
    "words = ['is', 'it', 'good', '?']\n",
    "tmp = words[0]\n",
    "words[0] = words[1]\n",
    "words[1] = tmp\n",
    "words[3] = '!'\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Using tuples\n",
    "\n",
    "# Transform to ['it', 'is', 'good', '!']\n",
    "\n",
    "words = ('is', 'it', 'good', '?')\n",
    "words_tuple = (words[1], words[0], words[2], '!')\n",
    "list(words_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Write code that removes whitespace at the beginning and end of a string ('   this   is a sample  sentence '), and normalizes whitespace between words to be a single space character.\n",
    "# a) do this task using split() and join()\n",
    "# b) do this task using regular expression substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) do this task using split() and join()\n",
    "\n",
    "sent = '   this   is a sample  sentence '\n",
    "' '.join(sent.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) do this task using regular expression substitutions\n",
    "\n",
    "import re\n",
    "re.sub(r'\\s+', ' ', re.sub(r'^\\s+|\\s+$', '', sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. sent1=['The', 'dog', 'gave', 'John', 'the', 'newspaper']. Now assign sent2=sent1. Modify sent1[1]='monkey'. Please review section 4.1 -Assignment in Chapter 4 to answer the following questions:\n",
    "# a) verify that sent2 has changed\n",
    "# b) Now try the same exercise but instead assign sent2=sent1[:]. Modify sent1[1]='monkey' and see what happens to sent2. Explain.\n",
    "# c) Now define text1=[['The', 'dog', 'gave', 'John', 'the', 'newspaper'], ['John', 'is', 'happy']]. Now assign text2=text1[:], assign a new value to one of the words (text1[0][1]='monkey'). Check what happens to text2. Explain.\n",
    "# d) Extract successive overlapping 4-grams from ['The', 'dog', 'gave', 'John', 'the', 'newspaper']. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "\n",
    "sent1=['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "sent2=sent1\n",
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1[1]='monkey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1==sent2)\n",
    "\n",
    "# sent2 has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can see that sent 2 has changed. The second element in the list has changed from dog to monkey as we can see above. The change is true for the original list sent1 as well. This is like creating aliases.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "\n",
    "sent1=['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "sent2=sent1[:]\n",
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1[1]='monkey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'monkey', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'gave', 'John', 'the', 'newspaper']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(sent1==sent2)\n",
    "\n",
    "# sent2 has not changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***sent1[:] creates a shallow copy of the original list into sent 2. But it does not refer to the same list object. Hence we don't risk changing the original list by changing the copy created by sent[:].***\n",
    "\n",
    "***NOTE: When reading, list is a reference to the original list, and list[:] shallow-copies the list.***\n",
    "\n",
    "***Explanation: The sent2 keeps a reference of the original items and any changes to the sent1 now is not refelected on sent2. That's exactly what we see above. This is a shallow copy but since the list obejct here is NOT compunded it also acts as if it is a deep copy and thus sent2 is no longer same as sent1.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c)\n",
    "\n",
    "text1=[['The', 'dog', 'gave', 'John', 'the', 'newspaper'], ['John', 'is', 'happy']]\n",
    "text2=text1[:]\n",
    "text1[0][1]='monkey'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'monkey', 'gave', 'John', 'the', 'newspaper'],\n",
       " ['John', 'is', 'happy']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'monkey', 'gave', 'John', 'the', 'newspaper'],\n",
       " ['John', 'is', 'happy']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(text1==text2)\n",
    "\n",
    "# text2 has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explanation: In this case it is a compound object (lists of lists) and this is exactly where the difference between shallow copy and deep copy comes into picture. In case of a compunded object like the one we have over here, it will create a copy of the inside list and thus any change will be reflected on both.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'dog', 'gave', 'John'],\n",
       " ['dog', 'gave', 'John', 'the'],\n",
       " ['gave', 'John', 'the', 'newspaper']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d)\n",
    "\n",
    "# Method 1:\n",
    "\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "n = 4\n",
    "[sent[i:i+n] for i in range(len(sent) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'dog', 'gave', 'John'),\n",
       " ('dog', 'gave', 'John', 'the'),\n",
       " ('gave', 'John', 'the', 'newspaper')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2:\n",
    "\n",
    "import nltk\n",
    "list(nltk.ngrams(sent,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Write a function that prints any word that appeared in the last 20% of a text that had not been encountered earlier. Use text1 from nltk.book to call this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "# I thought I can run the set function to identify the unique values from one list and from that\n",
    "# just subtract the ones from another list against which we want to compare.\n",
    "\n",
    "result = []\n",
    "x = ['I','like','NLP','class']\n",
    "y = ['Which','class','do','you','like']\n",
    "for item in set(x) - set(y):\n",
    "    result.append(item)\n",
    "    \n",
    "print(result)\n",
    "        \n",
    "# Here we can see it returns -- 'I and NLP' as words from set x because 'like and class' were present in set y too.\n",
    "# So I need to just include this inside a fucntion and replace \n",
    "# x with last 20% of words from text1 and y with first 80% of words from text1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "def last20(text):\n",
    "    limit = int(0.8*len(text))\n",
    "    for word in set(text[limit:]) - set(text[:limit]):\n",
    "        result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crazed', 'dictating', 'Shake', 'prolongingly', 'patris', 'unstricken', 'BLACKSMITH', 'Horatii', 'Manx', 'Bring', 'lookouts', 'impresses', 'Unshored', 'HATCHWAY', 'THY', 'supernal', 'Labor', 'bewitching', 'Candles', 'convalescence', 'sneezing', \"';\", 'Science', 'griffins', 'herons', 'enchantment', 'babe', 'filial', 'IS', 'imminglings', 'penetrating', 'disencumber', 'cures', 'UPON', 'Pan', 'strings', 'soak', 'restlessness', 'vindictively', 'flambeaux', 'shave', 'PREVIOUS', 'relying', 'clover', 'roly', 'flume', 'upheaved', 'mistrust', 'Muffled', 'HANGING', 'forgets', 'expectant', 'Bashee', 'personality', 'Mother', \"!--'\", 'complies', 'transfixedly', 'ADDITIONAL', 'Pharaoh', 'couldst', 'incommodiously', 'reined', 'machine', 'Astern', 'sidelingly', 'shrunk', 'troubledly', 'exploding', 'waked', 'ineffaceable', 'COMES', 'uninterpenetratingly', 'designation', 'primogenitures', 'reluctantly', 'Rue', 'tyrannical', 'blessing', 'SNEEZING', 'Symphony', 'firmer', 'FAIR', 'gleaming', 'Fool', 'Gilder', 'spindled', 'geniuses', 'caged', 'attuned', 'bartered', 'cindered', 'flourishin', 'Poles', '.\"--', 'pledges', 'sash', 'technicals', 'insensible', 'sinned', 'countersinkers', 'disaffection', 'unset', 'outwards', 'older', 'groin', 'Mississippies', 'Relieved', 'shuts', 'BENCH', 'bigot', 'happenest', 'Aldrovandus', 'mud', 'compendious', 'acquiesced', 'fagot', 'stupidly', 'sinew', 'studs', 'VICE', 'Shem', 'frenzies', 'interfering', 'trample', 'Grief', 'tray', 'remind', 'moderns', 'Geological', 'padlocks', 'sickness', 'imperiously', 'transpire', 'moccasined', 'weld', 'numbed', 'Antwerp', 'destroying', 'TUBS', 'moaning', 'partiality', 'declining', 'yokes', 'passionlessness', 'despised', 'strays', 'siskur', 'oblivious', 'pirouetting', 'Rib', 'Begone', 'awls', 'End', 'temper', 'verbal', 'vane', 'coincidings', 'unappalled', 'Joy', 'LAID', 'Lucky', 'sneering', 'Measured', 'tug', 'islanders', 'dispute', 'runaway', 'rainbowed', 'abounded', 'Albemarle', 'consciously', 'prolongings', 'unmeaningly', 'arrant', 'Untried', 'shuttlecock', 'thoughted', 'partial', 'baffle', 'scrolled', 'Eternities', 'groom', 'metallic', 'trampled', 'spavined', 'Tropics', 'Glancing', 'musket', 'soliloquizes', 'Base', 'inscribed', 'plausible', 'masculine', 'Swiss', 'Hoveringly', 'blighted', 'guiltiness', 'Thundering', 'ignores', 'Detached', 'gropes', 'unsolved', 'damply', 'splinter', 'companion', 'mistiness', 'dissolve', 'trimmed', 'haunt', 'Hat', 'Kremlin', 'annihilating', 'scream', 'unfamiliar', 'Assuredly', 'exchanged', 'BOSOM', 'LENGTHWISE', 'Blind', 'Harto', 'pauselessly', 'tingles', 'Sicilian', 'sewing', 'fished', 'Tempered', 'attestation', 'Emblazonings', '129', 'glacier', 'Belated', 'familyless', 'aloud', 'elves', 'soulless', 'thwart', 'sings', 'conduits', 'electricity', 'sterned', 'impersonal', 'auction', 'ABOUT', 'slogan', 'questionings', 'bandbox', 'Needle', 'fusee', 'sooth', 'diluvian', 'FILING', 'gamy', 'splintering', 'smacks', 'apron', 'methodic', 'vivacity', 'tri', 'unhappy', 'Admirals', 'miller', 'Power', 'unforgiven', '130', 'WORK', 'carelessness', 'adopting', 'tarnishing', 'lipless', 'ungraduated', 'fitfully', 'Strangest', 'Throttling', 'lightens', 'joyousness', 'FLASK', 'shading', 'Lackaday', 'cohered', 'assassins', 'intertangled', 'slumberers', 'vindicated', 'dumbest', 'unmeasured', 'Gliding', 'transferringly', 'placeless', 'despot', 'cocked', 'longed', 'citadels', 'untraceable', 'splendors', 'rip', 'suck', 'intermission', 'splashing', 'drills', 'orisons', 'Ark', 'muffle', 'Ego', 'discharges', 'inwreathing', 'Helm', '131', 'rifles', 'Jesus', 'nomine', 'Snap', 'blacksmith', 'marl', 'prescience', 'ventilated', 'suicide', 'Loaded', 'Buoy', 'sunshine', 'undertake', 'Either', 'windbound', 'NINE', 'aptitudes', 'lizard', 'comprehensively', 'ledgers', 'huskily', 'extremities', 'quadrant', 'unsuffusing', 'slouching', 'Slope', 'maliciously', 'flyin', 'Antilles', 'exhumed', 'ravening', 'sugary', 'Solander', 'apeak', 'billed', 'interchangeably', 'RED', 'select', 'mulberries', 'unwinking', 'touchest', 'CARPENTER', 'Overhearing', 'dazed', 'Hannibal', 'racing', 'receptive', 'Magian', 'creativeness', 'dew', 'hid', 'Ixion', 'poly', 'vales', 'prostration', 'tiered', 'chargers', 'clustered', 'buttress', 'mutations', 'grapple', 'thief', 'liest', 'SCENE', 'playfully', 'saving', 'oozed', 'unreverenced', 'hummingly', 'obscures', 'unintelligence', 'fleeces', 'pettiness', 'sites', 'SEEN', 'insultest', 'involving', 'pertained', 'TIMES', 'trodden', 'liv', 'pricked', 'bitterer', 'Fields', 'Alike', 'unsetting', 'ESCAPED', 'lotions', 'unconcluded', 'hammering', 'bubblingly', 'Bellies', 'Cursed', 'strandings', 'Rains', 'Rocks', 'compliance', 'bulge', 'epaulets', 'FROCK', 'watchman', 'wrung', 'binding', 'bids', 'Canadian', 'Yoke', 'Mene', 'petition', 'fossils', 'Brace', 'Cooke', 'catacombs', 'slate', 'abstraction', 'Watery', 'inextricable', 'honesty', 'precipitating', 'banned', 'inflexibly', 'listened', 'unsmoothable', 'creeps', 'manipulator', 'ebb', 'destroyer', 'sieve', 'Lightning', 'rotten', 'crusts', 'degenerated', 'sheathed', 'ascertaining', 'populousness', 'vermillion', 'Against', 'Mend', 'eastwards', 'Warmest', 'parched', 'Cruppered', 'horizons', 'Evangelist', 'etherial', 'crouches', 'scowled', 'footstep', 'Crushed', 'placid', 'truer', 'stereotype', 'gait', 'unsourced', 'comber', 'sinning', 'FIXED', 'genealogy', 'wanings', 'concurred', 'GOES', 'Sciences', 'bobbed', 'Cherries', 'tubes', 'baser', 'splits', 'boyhood', 'thereupon', 'cobbler', 'apparel', 'plungingly', 'wittiness', '125', 'prone', 'bullied', 'foothold', 'complacent', 'braver', 'sacramental', 'hems', 'favourites', 'caulked', '121', 'immutably', 'drums', 'impeding', 'innocently', 'clumsiest', 'miserly', 'Launched', 'caking', 'tangles', 'heigh', 'bodiless', 'punctually', 'fainter', 'abstract', 'brazen', 'STEPS', 'bedsteads', 'rivet', 'nasty', 'senate', 'cobbling', 'clumsily', 'impotence', 'artisan', 'Remote', 'frail', 'tantalization', 'Commodores', 'creamed', 'Isles', 'unstaggering', 'spool', 'conjure', 'recluseness', 'humbly', 'Marine', 'abstracted', 'namelessly', 'orphan', 'Basilosaurus', 'blindfold', 'triply', 'unmisgiving', 'Imprimis', 'tick', 'suction', 'ditchers', 'blazed', 'semiweekly', '105', 'coffins', 'Work', 'tugging', 'draggingly', 'swathed', 'adamite', 'outstretching', 'manmaker', 'welding', 'hymns', 'Trade', 'sobbings', 'receding', 'hooroosh', 'misnamed', 'longings', 'dirgelike', 'closing', 'Feel', 'halfspent', 'overleap', 'embalming', 'overspread', 'untottering', 'powdered', 'gores', 'antichronical', 'callest', 'disbelief', 'machines', 'vicissitude', 'gravestone', 'wanderer', 'guarantees', 'undertaker', 'Said', 'wrench', 'forbid', 'militia', 'rapture', 'THRICE', '117', 'plugged', 'grated', 'crafty', 'explanatory', 'rechristened', 'paddling', 'analysis', 'Clay', 'halloo', 'wheresoe', '115', 'czar', 'homewardbound', 'reaches', 'Ding', 'evolutions', 'honourably', 'malady', 'forging', 'Carpenter', 'Plenty', 'Africans', 'fertilely', 'limits', 'caulk', 'guide', 'libertines', 'fated', 'tempering', 'weaker', 'Unwittingly', 'displaced', 'grizzly', 'glidest', 'predestinating', 'freshened', 'sunwards', 'fadest', 'dispel', 'brags', 'critically', 'invincible', 'Quadrant', 'arranging', 'batten', 'rowlocks', 'Nail', 'encore', 'enticings', 'subordinates', 'contusions', 'posted', 'yawing', 'tipped', 'boomed', 'unintegral', 'Forge', 'perishable', 'Smithfield', 'OAKUM', 'FIRMLY', 'immeasurable', 'athwartships', 'SCREWS', 'gratitude', 'Art', 'MAIN', 'moons', 'fattest', 'trinity', 'baptism', 'evolution', 'troughs', 'tat', 'Delta', 'clefts', 'dong', 'suckled', 'bier', 'rousing', 'haphazard', 'clearest', 'uncontinented', 'capstans', 'misdoubt', 'Clinging', 'Rarmai', 'PLACED', 'Help', 'Inasmuch', 'Future', 'fading', 'OH', 'resent', 'ladders', 'pertinacious', 'idiotic', 'retracing', 'rumbled', 'corridors', 'wavings', 'collect', 'abbreviate', 'streamers', 'glitteringly', 'tape', 'cherry', 'infatuation', 'Sikoke', 'intertwistings', 'corpusants', 'Camel', 'cankerous', 'tyrant', 'placards', 'Furl', 'LINE', 'LEATHER', 'woody', 'te', 'mayst', 'boastful', 'sustains', 'Shrouded', 'vouchers', 'Historians', 'justify', 'Hah', 'HARD', 'bowsmen', 'Log', 'shiningly', 'vassal', 'overspreading', 'teeter', 'inboard', 'Ephesian', 'pinch', 'underling', 'blister', 'TOOLS', 'portent', 'clamped', 'cowardly', 'loosed', 'sultanically', 'bladders', 'Shame', '108', 'bordering', 'transpointed', 'unfavourable', 'mixing', 'Hadst', 'magnetizing', 'banked', 'SHALL', 'nostril', 'filers', 'intensities', 'pattern', 'hallo', 'Sheffield', 'nestling', 'Truly', 'shooks', 'Scotland', 'deadreckoning', 'scorchingly', 'underived', 'loadstone', 'wails', 'strenuous', 'allotted', 'PASSING', 'swiftest', 'fists', 'Niphon', 'moonlit', 'Lama', 'wondrousness', 'Sleeping', 'statistically', 'outbranching', 'standest', 'chipping', 'poky', 'Wife', 'relations', 'tweezers', 'gloriously', 'fitful', 'YARD', 'molten', 'Leaning', 'sagged', 'Upharsin', 'plumes', 'hares', 'diaboli', 'AROUND', 'Monadnock', 'manes', 'incuriously', 'MULTUM', 'unrigged', 'falsified', 'scamp', 'acute', 'unsuspecting', 'dams', 'reverberations', 'cruellest', 'spill', 'hued', 'PRESSED', 'manifestation', 'reeved', 'Thrusting', 'aromas', 'Tekel', 'bladed', 'Diving', 'chucks', 'intermixture', '122', 'TWISTED', 'dawned', 'trebly', 'bystanders', 'Retribution', 'EYES', 'subservient', 'outraged', 'Deliberately', 'rugged', 'conspired', 'firmaments', 'Epilogue', 'CATCHES', 'MEN', 'manoeuvred', 'Tarquin', 'moulder', 'negatively', 'obeyest', 'seconds', 'transfixed', 'pain', 'cradled', '118', 'cabalistical', 'Madman', '106', 'signers', 'enjoyments', 'unfractioned', 'railing', 'Heart', 'relent', 'handicrafts', 'synod', 'core', 'trailing', 'peopled', 'adown', 'Matsmai', 'importunity', 'fours', 'LIGHTNING', 'mason', 'launchest', 'boon', 'bodings', 'functionary', 'wert', 'REST', 'Californian', 'Heave', 'caulking', 'Gardiner', 'tongues', 'lawful', 'bedraggled', 'forgive', 'leaner', 'heartwoes', 'clappings', 'stall', 'corkscrewed', 'revery', 'togs', 'pleases', 'scolds', 'Fashioned', 'awed', 'arrowy', 'Loftiest', 'gnawed', 'Anatomist', 'slink', '1842', 'loon', 'um', 'shrinking', 'evanescence', 'Shiver', 'APPROACHING', 'pennons', 'salmon', 'purr', 'swamping', 'fiercer', 'SLABS', 'judicious', 'blockhead', 'flailed', 'soliloquizing', 'superiors', 'soars', 'Lifted', 'oversailed', 'savannas', 'earthy', 'hollowly', 'brats', 'recovers', 'extinction', 'treble', 'uncompromisedness', 'wrestling', 'poise', 'stirrings', 'unretracing', 'undignified', 'slunk', 'awestruck', 'sed', 'hoping', 'spindle', 'intercepted', 'intending', 'convalescing', 'drenching', 'Zeuglodon', 'unwinding', 'braced', 'Burtons', 'LIGHT', 'JOIST', 'forge', 'toy', 'Rat', 'liberated', 'craters', 'BUSILY', 'panel', 'owe', 'intermixingly', 'stunning', 'partnership', 'potentates', 'gaffs', 'exasperate', 'knotty', 'OPEN', '111', 'motionlessly', 'Till', 'uneven', 'Flat', 'Dragged', 'TURNS', 'Himmalehs', 'Asphaltites', 'Started', 'Bungle', 'swooping', 'callings', 'spearings', 'THEE', 'wailings', 'revolvingly', 'foreshortened', 'PADS', 'FLAME', 'bellows', 'blithe', 'unmomentous', 'Magnitude', 'Archipelagoes', 'Silent', 'aisle', 'erring', 'adoption', 'remunerative', 'rechurned', 'midmost', 'transparently', 'concocts', ';\"--(', 'hickory', 'brush', 'railway', 'slower', 'architecture', 'fittings', 'Leeward', 'thistle', 'entreated', 'unconquering', 'slaughtering', 'MOUNTED', 'somnambulisms', 'tack', 'promptitude', 'glue', 'elm', 'efficient', 'tastin', 'lava', 'Mahomet', 'Supreme', 'coincident', 'Welding', 'lovings', 'Comparing', 'ENSUING', 'bequeathed', 'belayed', 'keener', '134', 'backwardly', 'jealously', 'dusk', 'gap', 'exterminated', 'siding', 'oneness', 'Impiety', 'IVORY', 'attaching', 'frenzied', 'mothered', 'Conjuror', 'barbarian', 'frothed', 'spicin', 'Louisiana', 'shallop', 'propped', 'foamin', 'sex', 'waxes', 'mutes', 'thinned', 'phosphorescence', 'UNWINDING', 'heartlessness', 'whimsiness', 'blotted', 'stubbs', 'weeps', 'elder', 'Carey', 'fatalistic', 'partook', 'sprains', 'knitted', 'combinedly', 'wept', 'Academy', 'Owners', 'torch', 'trysail', 'Matter', 'debtor', 'rover', 'procedures', 'fluttering', 'circumferences', 'lucifers', 'FORGE', 'lesser', 'ironical', 'plough', 'Beams', 'spontaneous', 'volley', 'ungovernable', 'wasps', 'Dauphine', 'Ahabs', 'crashing', 'replace', 'Tanaquil', 'thrilled', 'dallied', 'chickens', 'outyell', 'Samson', 'madden', 'stealthily', 'holders', 'HELM', 'joky', 'shinbone', 'Hearkening', 'grotesque', 'perpetuates', 'chock', 'guilt', 'felicities', 'lustily', 'baulks', 'mobbing', 'befooled', 'crest', 'Loveliness', 'scornest', 'Perish', 'crushing', 'resemblances', 'basketed', 'sourceless', 'jubilations', 'candid', 'Bachelor', 'muster', 'expertness', 'jettest', 'defiance', 'Americas', 'determining', 'marsh', 'Penetrating', 'STEP', 'Dead', 'preventer', 'economic', 'SLOWLY', 'oval', 'affirm', 'wincing', 'aslope', 'Common', 'POKE', 'gambol', 'pitchers', 'moles', 'dartingly', 'treading', 'urgent', 'inhabitable', 'Light', 'characterized', 'unaccounted', 'unbegotten', 'Boats', 'ROLL', 'writhed', 'reforming', 'lasts', 'bewildering', '112', 'affinities', 'Ifs', 'clearness', 'geologist', 'Europa', 'drama', 'Whose', 'attentive', 'omnitooled', 'belaying', 'Tunnel', 'bejuggled', '123', 'blunted', 'digger', 'unmanned', 'reelingly', 'unprepared', ';--\"', 'blanched', 'cloudless', 'workers', 'soliloquizer', 'bunting', 'moodily', 'contrastingly', 'crumb', 'trained', 'singed', 'sweetness', 'chill', 'mastery', 'favouring', 'numbness', 'colic', 'catcher', 'unearthed', 'REPEATED', 'uncatastrophied', 'insolent', 'fallacious', 'pedlar', 'contributory', 'determine', '126', 'ditches', 'supplication', 'undue', 'comforts', 'ravines', 'mixes', 'nutmeg', 'sweeps', 'shears', 'Miles', 'woeful', 'smoothed', 'metres', 'Walks', 'afire', 'LANTERNS', 'canals', 'whaleships', 'unbegun', 'Japans', 'bluer', 'foster', 'judges', 'surveying', 'animate', 'wrapping', 'birch', 'feathering', 'LASHINGS', 'thinkest', 'Albicore', 'Monsieurs', 'Tahitian', 'launch', 'queenly', 'bomb', 'abhorring', 'Inferable', 'kindling', 'shortest', 'continents', 'stolidity', 'HEIGHT', 'interflow', 'loosening', 'THESE', 'Typhoon', 'hawk', 'domineered', 'fuller', 'Unmindful', 'forbearing', 'floes', 'weasel', 'repairing', 'Sway', 'incommoding', 'negroes', 'unfold', 'jobs', 'foreknew', 'hooped', 'ether', 'strivest', 'cymballing', 'display', 'unmoor', 'homes', 'jetting', 'inuendoes', 'crucible', 'lieutenant', 'suffered', 'bristling', 'blistered', 'prescient', 'Despatch', 'Lad', 'centrepiece', 'Instances', 'razors', 'mocked', 'Chase', 'slavery', 'sulphurous', 'initiate', 'Mate', 'expert', 'leapest', 'Dying', 'Hither', 'coasting', 'pertinaciously', 'busying', 'unmanageably', 'heedlessly', 'GANGWAY', 'survive', 'asunder', 'Heading', 'woodpecker', 'jointed', 'foregone', 'doctored', 'outspread', 'unsignifying', 'THUS', 'lullaby', 'poled', 'caress', 'convent', 'unsurrendered', 'slided', 'hydrants', 'CLOSES', 'treasuries', 'SEAMS', 'Lashed', 'dinning', 'marge', 'Tertiary', 'Caught', 'Crete', 'wring', 'rulers', 'furrow', 'warped', 'mockery', 'paternal', 'tiers', 'servile', 'Antiochus', '109', 'Burst', 'Mogulship', 'speakest', 'clingings', 'tinkers', 'tilting', 'flaw', 'haggardly', 'whencesoe', 'frayed', 'mummeries', 'Level', 'markest', 'heaviness', 'Believe', 'tomorrow', 'eloped', 'sporty', 'monotonously', 'tauntingly', 'scorch', 'surpass', 'vesper', '110', 'cachalot', 'gaseous', 'ARE', 'gulping', 'Pressing', 'pall', 'tracings', 'impersonated', 'unastonished', 'lave', 'disinterred', 'brokenly', 'Rafters', 'outlast', 'extermination', '132', 'Jerk', 'Invisible', 'HO', 'prating', 'whirls', 'Tying', '116', 'indebtedness', 'cured', 'nearing', 'Herculaneum', 'RESUMING', 'Hurriedly', 'panels', 'sizes', 'discerned', 'duplicates', 'pore', 'pensive', 'Satanic', 'spectrally', 'actest', 'smoothe', 'Aroostook', 'tindering', 'antemosaic', 'betters', 'FLAMES', \"',--\", 'unsweetly', 'plaid', 'shortened', 'bounding', 'Methuselah', 'breaching', '133', 'apprise', 'mediums', 'crape', 'conductor', 'Goa', 'Miracle', 'Typhoons', 'pincers', 'sisterly', 'bleaching', 'DURING', 'blueness', 'undiscoverable', 'astrological', 'Abashed', 'ITS', 'kine', 'nuptial', 'inter', 'mishaps', 'icily', 'Soothed', 'gleam', 'grip', 'basement', 'conflicting', 'adolescence', 'omens', 'alternate', 'lover', 'LEG', 'loweringly', 'disembowelled', 'unappeasable', 'uncracked', 'grows', 'doored', 'heliotrope', 'shudderings', 'fadeless', 'CAULKING', 'planetarily', 'sobbing', 'Bad', 'Um', 'modifying', 'PARVO', 'lapsed', 'punctures', 'probed', 'chronically', 'mattrass', 'genially', 'foamingly', 'separable', 'fidelities', 'forsaken', 'progressive', 'immaterial', 'whelmings', 'unblinkingly', 'poorest', 'upraising', 'wells', 'wan', 'FORWARD', '124', 'seekest', 'Tunnels', 'dampness', 'cisterns', 'feebly', 'rejection', 'compunctions', 'invasion', 'dinnerless', 'rustling', 'pallidness', 'modelled', 'reliance', 'heron', 'howlings', 'coppers', 'mingling', 'plaintively', 'alluringly', 'dislodged', 'enacted', 'lieutenants', 'clustering', 'osseous', 'scorches', 'exclusiveness', 'teetering', 'foundling', 'spiralized', 'houseless', 'imponderable', 'abided', 'mastering', 'feebler', 'Potters', 'droves', 'bundles', 'baptismal', 'proverbial', 'Excellent', 'baptizo', 'CHEERLY', '113', 'Forehead', 'tainted', 'forbids', 'pang', 'disappearance', 'Buoyed', 'Semiramis', 'copies', 'Rachel', 'inspecting', 'shunning', 'erectly', 'STRING', 'innocency', 'warningly', 'rearward', 'evolved', 'Niger', 'Seek', 'unparticipated', 'emptying', '107', 'Groan', 'cherrying', '114', 'neglect', 'confirmation', 'missent', 'unfrequented', 'streaming', '.--\"', 'Babylonian', 'maple', 'philosophies', 'Diminish', 'upcast', 'peals', 'arrange', 'Netherlands', 'bastions', 'risks', 'tinkerings', 'eave', 'Ripplingly', 'pointest', 'cowards', 'plaything', 'screws', 'stratagem', 'Denderah', 'skewered', 'Formosa', 'planisphere', 'wee', 'amputate', 'unnamable', 'vibrate', 'slim', 'FLASHES', 'inanimate', 'Thrusted', 'Put', 'watergate', 'stunsails', 'chalking', 'overstrained', 'ancestry', 'Innocents', 'Forward', 'postponedly', 'ANCHORS', 'sinecures', 'Maccabees', 'print', 'premeditated', 'Accessory', 'pointless', 'Signals', 'curls', 'Immortal', 'childlessness', 'Herod', 'sayst', '135', 'Set', 'unhesitatingly', 'CONTINUES', 'SUDDEN', 'competency', 'MOVING', 'pedigree', 'mermaids', 'Sailing', 'ancestors', '1779', 'quicksilver', 'Omen', 'cinders', 'FOLLOWING', 'individualities', 'imagining', 'longevity', 'Weep', 'Paris', 'Wrinkled', 'exertions', 'puncheons', 'Unobserved', 'derision', 'ventured', 'veer', 'excludes', 'lowermost', 'Same', 'omniscient', 'voiced', 'tableau', 'enticing', 'feverish', 'oddly', 'canonic', 'inquired', 'Befooled', 'Winds', 'Blacksmith', 'Availing', 'weariest', 'cherries', 'SORTS', 'lusty', 'hearses', 'Making', 'iciness', 'gluepots', 'overtaking', 'cape', 'Devils', 'ferrule', 'investigation', 'forebodings', 'limestone', 'influenced', 'outbellying', 'shank', 'TOP', 'songster', 'ding', 'Stir', 'orchestra', 'conceals', '!--\"', 'hoky', 'gradual', 'plaintiveness', 'Cold', 'unspoken', 'Curse', 'wheezing', 'Horse', 'Accursed', 'bravadoes', 'obeys', 'unverdured', '128', 'jesty', 'intermeddling', 'archangelic', 'contributed', 'Skies', 'fulfilment', 'molesting', 'reelman', 'pruning', 'charter', 'Saw', 'poser', 'roundly', 'marbleized', 'staved', 'unstranded', 'WHERE', 'assail', 'frenzy', 'clouded', 'Seat', 'Bulwarks', 'inactive', 'musical', 'seniors', 'Tierce', 'magnetism', 'huff', 'billion', 'plaits', 'seamless', 'fang', 'leakage', 'creative', 'charcoal', 'ravished', 'Praetorians', 'Bones', 'glittered', 'ONLY', 'LL', 'Leap', 'hemlock', 'unmurmuringly', 'drunkard', 'unstreaked', 'drat', 'privation', 'stacking', 'harp', '119', 'eavesdroppers', 'subterraneousness', 'presaged', 'palpableness', 'suspense', 'unspeckled', '120', 'rats', 'peeringly', 'Foolish', 'stowage', 'shame', 'Jumped', 'flattening', 'darkling', 'tiny', 'holiness', 'skimming', 'lotion', 'struggled', 'unenervated', 'interpenetrate', 'disabled', 'befall', 'squadrons', 'AM', 'watchmen', 'Gone', 'Creagh', 'thanks', 'wink', 'drench', 'withholding', 'bowstring', 'Fata', 'damped', 'centipede', 'neighborhood', 'softness', 'sod', 'ramifying', 'leewardings', 'seams', 'reverse', 'shop', 'ejaculated', 'madest', 'vortex', 'Saturn', 'twitching', 'deduction', 'flickering', 'remonstrance', 'cats', 'vividness', 'daft', 'HIMSELF', 'virtuous', 'paternity', 'Met', 'wealthiest', 'transfix', 'gnarled', 'annually', 'HEARS', 'glimmering', 'forethrown', 'aleak', 'sup', 'winces', 'unimpressed', 'finish', 'acuteness', 'Zoroaster', 'brushwood', 'riddle', 'automaton', 'knell', 'pulls', 'collecting', 'weak', 'regiments', 'STRAPS', 'disguisement', 'intercept', 'gradations', 'hardihood', 'fairer', 'chafed', 'monsieurs', 'intercedings', 'lurching', 'foreshadowing', 'branding', 'luff', 'demijohn', 'feelest', 'landscapes', 'Perth', 'Musket', 'aspiring', 'persecutions', 'tetering', 'voicelessly', 'Bastille', 'revelation', 'oust', 'unchangeable', 'holier', 'goest', 'wantonly', 'recrossed', 'Drat', 'fastenings', 'believers', 'decree', 'shutter', 'husks', 'drivers', 'thrill', 'unwedded', 'enslaved', 'SNEEZES', 'check', 'envied', 'reydan', '127', 'jerkingly', 'Morgana', 'murmur', 'unwound', 'inequality', 'supernaturalness', 'wrinkling', 'focus', 'Sleep', 'Petrified', 'thoughtless', 'snarls', 'stubbornest', 'dissolved', 'formations', 'lightness', 'literal', 'foretell', 'SAIL', 'Smut', 'unyielding', 'unfeatured', 'tranquillities', 'passionately', 'Fired', 'wooing', 'anyone', 'trending', 'dejected', 'stilly', 'Titans', 'OVER', 'commandingly', 'outspreadingly', 'breathest', 'Channel', 'abate', 'layn', 'nautilus', 'recentest', 'BETWEEN', 'centres', 'fossil', 'persuasion', 'shrunken', 'untrackably', 'barriers', 'responsibilities', 'Bone', 'bannered']\n"
     ]
    }
   ],
   "source": [
    "print(last20(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Write a program that takes the sentence (\"we have seen two kinds of sequence object: strings and lists.\")expressed as a single string, splits it and counts up the tokens. Get it to print out each token and the token's frequency, one per line, in alphabetical order. You should write a function and call that function to process the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"we have seen two kinds of sequence object: strings and lists.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sent(text):\n",
    "    fdist = FreqDist(w.lower() for w in nltk.word_tokenize(text))\n",
    "    for dict_key in sorted(fdist.keys()):\n",
    "        print(\"Frequency of the token: {0:<8} is {1}\".format(dict_key, fdist[dict_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of the token: .        is 1\n",
      "Frequency of the token: :        is 1\n",
      "Frequency of the token: and      is 1\n",
      "Frequency of the token: have     is 1\n",
      "Frequency of the token: kinds    is 1\n",
      "Frequency of the token: lists    is 1\n",
      "Frequency of the token: object   is 1\n",
      "Frequency of the token: of       is 1\n",
      "Frequency of the token: seen     is 1\n",
      "Frequency of the token: sequence is 1\n",
      "Frequency of the token: strings  is 1\n",
      "Frequency of the token: two      is 1\n",
      "Frequency of the token: we       is 1\n"
     ]
    }
   ],
   "source": [
    "process_sent(text)\n",
    "\n",
    "# It is arranged in alphabetical order of dict_key as asked in the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Write a function shorten(raw, n) to process a text ('big big world today tomorrow good Today good'), omitting the n most frequently occurring words of the text. You should use w.lower() to normalize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'big big world today tomorrow good Today good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten(raw, n):\n",
    "    words = nltk.word_tokenize(raw.lower())\n",
    "    fdist = FreqDist(w for w in words if w.isalpha())\n",
    "    tmp = {w for w, freq in fdist.most_common(n)}\n",
    "    revised_text = ' '.join(w for w in words if w not in tmp)\n",
    "    return revised_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world today tomorrow good today good'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(text,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world tomorrow good good'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(text,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'world tomorrow'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(text,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tomorrow'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(text,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorten(text,5)\n",
    "\n",
    "# Any number of n > 5 will gives us a blank.\n",
    "# This is important as it keeps removing the blank and replaces it with a new blank.\n",
    "# There would thus be no error for shorten(text,6), shorten(text,7) and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explanation: I created a function to read the frequency distribution of words which are alphabetic and then created a tmp variable to store the most common words. They are stored in the order of the input, so we can call it for n=1 then even if there is a tie it will omit the top most priority in the given sequence as available in text. This tmp variable acts as a pseudo index and when we run the shorten () function with some value for n, it will remove all the values until that count and return the remianing ones.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Write a list comprehension that sorts a list of WordNet synsets for proximity to a given synset. For example, given the synsets lesser_rorqual.n.01, killer_whale.n.01, novel.n.01, and tortoise.n.01, sort them according to their shortest_path_distance() from right_whale.n.01.You should use lambda in the sorted() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('lesser_rorqual.n.01'),\n",
       " Synset('killer_whale.n.01'),\n",
       " Synset('novel.n.01'),\n",
       " Synset('tortoise.n.01')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_synsets = [wn.synset('lesser_rorqual.n.01'), wn.synset('killer_whale.n.01'), wn.synset('novel.n.01'), wn.synset('tortoise.n.01')]\n",
    "wn_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('lesser_rorqual.n.01')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_synsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest Path Distance of Synset('lesser_rorqual.n.01') from Synset('right_whale.n.01') is 3\n",
      "Shortest Path Distance of Synset('killer_whale.n.01') from Synset('right_whale.n.01') is 5\n",
      "Shortest Path Distance of Synset('novel.n.01') from Synset('right_whale.n.01') is 22\n",
      "Shortest Path Distance of Synset('tortoise.n.01') from Synset('right_whale.n.01') is 12\n"
     ]
    }
   ],
   "source": [
    "distance = [synset.shortest_path_distance(wn.synset('right_whale.n.01')) for synset in wn_synsets]\n",
    "\n",
    "for i in range (len(distance)):\n",
    "    # print(\"Shortest path distance of {0:<8} from right_whale.n.01 is {1}\".format(wn_synsets[i], distance[i]))\n",
    "    print(\"Shortest Path Distance of {0} from Synset('right_whale.n.01') is {1}\".format(wn_synsets[i], distance[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('lesser_rorqual.n.01'),\n",
       " Synset('killer_whale.n.01'),\n",
       " Synset('tortoise.n.01'),\n",
       " Synset('novel.n.01')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(wn_synsets, key=lambda x: x.shortest_path_distance(wn.synset('right_whale.n.01')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explanation: When we run the sorted function with lambda I can see that the sequence is refreshed based on the shortest distance from right_whale.n.01 and now tortoise comes before novel.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Write a function that takes a list of words (containing duplicates) (i.e. words=['table','chair','desk','table','table','chair']) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g. if the input list contained 10 instances of the word table and 9 instances of the word chair, then table would appear before chair in the output list.You should use lambda in the sorted() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table', 'chair', 'desk', 'table', 'table', 'chair']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given example:\n",
    "\n",
    "words=['table','chair','desk','table','table','chair']\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table', 'chair', 'desk']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1: Using lambda in the sorted () as asked in the question\n",
    "\n",
    "def remove_dup(words):\n",
    "    fdist = FreqDist(words)\n",
    "    return sorted(set(words), key=lambda x:fdist[x], reverse=True)\n",
    "remove_dup(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table', 'chair', 'desk']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2: Using simple list comprehension with most_common method\n",
    "\n",
    "def remove_dup_2(words):\n",
    "    fdist = FreqDist(words)\n",
    "    return [w for w, fd in fdist.most_common()]\n",
    "remove_dup_2(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert elements of your choice. Please ensure each element is seperated by a space\n",
      "Your input --> table table table chair chair desk book book book book book\n",
      "['book', 'book', 'book', 'book', 'book', 'chair', 'chair', 'desk', 'table', 'table', 'table']\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Let user input the elements and then based on frequency of the elements in the list run the remove_dup function from method 1\n",
    "\n",
    "print(\"Insert elements of your choice. Please ensure each element is seperated by a space\")\n",
    "x = input(\"Your input --> \")\n",
    "word_list = x.split(' ')\n",
    "result = list(word_list)\n",
    "result.sort()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['book', 'table', 'chair', 'desk']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_dup(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a function that takes a text (e.g. text3 from nltk.book) and a vocabulary (e.g. nltk.corpus.words.words()) as its arguments and returns the set of words that appear in the text but not in the vocabulary. Both arguments can be represented as lists of strings. Can you do this using set().difference()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vs_vocabulary(text, vocabulary):\n",
    "    return set(text).difference(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I', 'NLP'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's verify this for a small set:\n",
    "\n",
    "text_vs_vocabulary(['I','like','NLP','class'], ['Which','class','do','you','like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arise',\n",
       " 'asked',\n",
       " 'How',\n",
       " 'Arphaxad',\n",
       " 'blesseth',\n",
       " 'Beeri',\n",
       " 'Bring',\n",
       " 'Deliver',\n",
       " 'Beersheba',\n",
       " 'Gather',\n",
       " 'comest',\n",
       " 'Two',\n",
       " 'Leummim',\n",
       " 'horsemen',\n",
       " 'Even',\n",
       " 'Sabtah',\n",
       " 'themselv',\n",
       " 'Iscah',\n",
       " 'ir',\n",
       " 'dea',\n",
       " 'Muppim',\n",
       " 'images',\n",
       " 'Amal',\n",
       " 'saith',\n",
       " 'favour',\n",
       " 'nostrils',\n",
       " 'avenged',\n",
       " 'wagons',\n",
       " 'ev',\n",
       " 'Eldaah',\n",
       " 'peop',\n",
       " 'sakes',\n",
       " 'badne',\n",
       " 'daughte',\n",
       " 'Es',\n",
       " 'moved',\n",
       " 'Save',\n",
       " 'Jerah',\n",
       " 'ri',\n",
       " 'rams',\n",
       " 'deceived',\n",
       " 'statutes',\n",
       " 'dunge',\n",
       " 'comi',\n",
       " 'gutters',\n",
       " 'Lotan',\n",
       " 'honour',\n",
       " 'Jamin',\n",
       " 'oth',\n",
       " 'eatest',\n",
       " 'dukes',\n",
       " 'destroyed',\n",
       " 'Thirty',\n",
       " 'sacrifices',\n",
       " 'ruled',\n",
       " 'Are',\n",
       " 'Elbethel',\n",
       " 'Shaul',\n",
       " 'Yea',\n",
       " 'pressed',\n",
       " 'At',\n",
       " 'Bless',\n",
       " 'tillest',\n",
       " 'Er',\n",
       " 'breaketh',\n",
       " 'Rosh',\n",
       " 'Shem',\n",
       " 'Whereas',\n",
       " 'Eber',\n",
       " 'Bela',\n",
       " 'Asenath',\n",
       " 'heard',\n",
       " 'Almodad',\n",
       " 'Oh',\n",
       " 'rebuked',\n",
       " 'Timna',\n",
       " 'Calah',\n",
       " 'They',\n",
       " 'preserved',\n",
       " 'despised',\n",
       " 'servants',\n",
       " 'Is',\n",
       " 'Say',\n",
       " 'Timnah',\n",
       " 'reproved',\n",
       " 'Naamah',\n",
       " 'Shalem',\n",
       " 'Malchiel',\n",
       " 'poured',\n",
       " 'overtook',\n",
       " 'chesnut',\n",
       " 'Pass',\n",
       " 'pleaseth',\n",
       " 'subtil',\n",
       " 'Manass',\n",
       " 'tak',\n",
       " 'reached',\n",
       " 'Ohad',\n",
       " 'Jobab',\n",
       " 'Baalhanan',\n",
       " 'Zuzims',\n",
       " 'Elam',\n",
       " 'dwe',\n",
       " 'sevens',\n",
       " 'counted',\n",
       " 'emptied',\n",
       " 'Shinar',\n",
       " 'sheweth',\n",
       " 'Zepho',\n",
       " 'Woman',\n",
       " 'circumcised',\n",
       " 'Shillem',\n",
       " 'Lest',\n",
       " 'Massa',\n",
       " 'Out',\n",
       " 'spices',\n",
       " 'walked',\n",
       " 'silv',\n",
       " 'grap',\n",
       " 'hills',\n",
       " 'Aner',\n",
       " 'became',\n",
       " 'Adbeel',\n",
       " 'Bedad',\n",
       " 'Gaza',\n",
       " 'Madai',\n",
       " 'thoughts',\n",
       " 'Temani',\n",
       " 'deprived',\n",
       " 'offered',\n",
       " 'Zoar',\n",
       " 'morter',\n",
       " 'multiplied',\n",
       " 'clusters',\n",
       " 'mouths',\n",
       " 'Eri',\n",
       " 'Mahanaim',\n",
       " 'Esek',\n",
       " 'spee',\n",
       " 'having',\n",
       " 'Ammon',\n",
       " 'faces',\n",
       " 'remembered',\n",
       " 'weaned',\n",
       " 'Gaham',\n",
       " 'Then',\n",
       " 'yielded',\n",
       " 'doeth',\n",
       " 'halted',\n",
       " 'handfuls',\n",
       " 'Give',\n",
       " 'riv',\n",
       " 'Jac',\n",
       " 'Achbor',\n",
       " 'archers',\n",
       " 'purchased',\n",
       " 'stories',\n",
       " 'shekels',\n",
       " 'anoth',\n",
       " 'Until',\n",
       " 'angels',\n",
       " 'These',\n",
       " 'voi',\n",
       " 'Adah',\n",
       " 'Kor',\n",
       " 'sawest',\n",
       " 'Mizz',\n",
       " 'Not',\n",
       " 'Hear',\n",
       " 'Tidal',\n",
       " 'Bethel',\n",
       " 'Atad',\n",
       " 'Mibzar',\n",
       " 'Where',\n",
       " 'heav',\n",
       " 'Tubalcain',\n",
       " 'countries',\n",
       " 'hunter',\n",
       " 'Up',\n",
       " 'kn',\n",
       " 'progenitors',\n",
       " 'handmaidens',\n",
       " 'Shur',\n",
       " 'elders',\n",
       " 'conceived',\n",
       " 'gr',\n",
       " 'Zerah',\n",
       " 'Jetheth',\n",
       " 'Serah',\n",
       " 'Shuni',\n",
       " 'LO',\n",
       " 'passed',\n",
       " 'My',\n",
       " 'searched',\n",
       " 'Mehetabel',\n",
       " 'stars',\n",
       " 'Gihon',\n",
       " 'pluckt',\n",
       " 'Jemuel',\n",
       " 'damsels',\n",
       " 'habitations',\n",
       " 'hid',\n",
       " 'Which',\n",
       " 'Eliezer',\n",
       " 'Am',\n",
       " 'liest',\n",
       " 'kid',\n",
       " 'younge',\n",
       " 'Ephah',\n",
       " 'seeth',\n",
       " 'Ithran',\n",
       " 'Edar',\n",
       " 'Shuah',\n",
       " 'befell',\n",
       " 'begettest',\n",
       " 'marvelled',\n",
       " 'observed',\n",
       " 'Spake',\n",
       " 'embraced',\n",
       " 'Abr',\n",
       " 'Esau',\n",
       " 'Who',\n",
       " 'Cursed',\n",
       " 'eyes',\n",
       " 'Forasmuch',\n",
       " 'rods',\n",
       " 'rul',\n",
       " 'Nod',\n",
       " 'Neither',\n",
       " 'speckl',\n",
       " 'Pau',\n",
       " 'reproa',\n",
       " 'aileth',\n",
       " 'Lasha',\n",
       " 'Thus',\n",
       " 'rained',\n",
       " 'Can',\n",
       " '.)',\n",
       " 'strengthened',\n",
       " 'Perizzit',\n",
       " 'Cast',\n",
       " 'lamentati',\n",
       " 'lighted',\n",
       " 'Abimael',\n",
       " 'Riphath',\n",
       " 'Hinder',\n",
       " 'Jokshan',\n",
       " 'Angel',\n",
       " 'hith',\n",
       " 'ribs',\n",
       " 'signs',\n",
       " 'womenservants',\n",
       " 'fruits',\n",
       " 'gro',\n",
       " 'Areli',\n",
       " 'Lamech',\n",
       " 'Admah',\n",
       " 'Nineveh',\n",
       " 'sinning',\n",
       " 'traffick',\n",
       " 'Siddim',\n",
       " 'shrubs',\n",
       " 'Hul',\n",
       " 'Se',\n",
       " 'vowedst',\n",
       " 'Heber',\n",
       " 'blessi',\n",
       " 'Arodi',\n",
       " 'Husham',\n",
       " 'Elparan',\n",
       " 'curseth',\n",
       " 'Lehabim',\n",
       " 'Guni',\n",
       " 'Blessed',\n",
       " 'hous',\n",
       " 'fainted',\n",
       " 'Ephron',\n",
       " 'Hiddekel',\n",
       " 'pulled',\n",
       " 'Jahzeel',\n",
       " 'hanged',\n",
       " 'healed',\n",
       " 'biteth',\n",
       " 'lovest',\n",
       " 'oa',\n",
       " 'Manasseh',\n",
       " 'boug',\n",
       " 'things',\n",
       " 'whales',\n",
       " 'fulfilled',\n",
       " 'Beerlahairoi',\n",
       " 'Upon',\n",
       " 'heads',\n",
       " 'Egy',\n",
       " 'likene',\n",
       " 'Zar',\n",
       " 'Sheba',\n",
       " 'sinners',\n",
       " 'Tarshish',\n",
       " 'branches',\n",
       " 'wives',\n",
       " 'darkne',\n",
       " 'travailed',\n",
       " 'pursued',\n",
       " 'tithes',\n",
       " \"'\",\n",
       " 'Escape',\n",
       " 'Hemam',\n",
       " 'earrings',\n",
       " 'Thou',\n",
       " 'ceased',\n",
       " 'troubled',\n",
       " 'Jabbok',\n",
       " 'Aram',\n",
       " 'Fifteen',\n",
       " 'Hadoram',\n",
       " 'Perizzites',\n",
       " 'Man',\n",
       " 'opened',\n",
       " 'Behold',\n",
       " 'lambs',\n",
       " 'Bow',\n",
       " 'waters',\n",
       " 'Resen',\n",
       " 'Hobah',\n",
       " 'Raamah',\n",
       " 'Lay',\n",
       " 'chode',\n",
       " 'Have',\n",
       " 'herdmen',\n",
       " 'butlers',\n",
       " 'Said',\n",
       " 'firmame',\n",
       " 'Tell',\n",
       " 'Sojourn',\n",
       " 'Phara',\n",
       " 'Feed',\n",
       " 'Beor',\n",
       " 'sle',\n",
       " 'mourned',\n",
       " 'fo',\n",
       " 'maidservants',\n",
       " 'Hadad',\n",
       " 'deeds',\n",
       " 'males',\n",
       " 'Lud',\n",
       " 'Ashkenaz',\n",
       " 'stretched',\n",
       " 'spi',\n",
       " 'famished',\n",
       " 'Dodanim',\n",
       " 'Enmishpat',\n",
       " 'Here',\n",
       " 'digged',\n",
       " 'wombs',\n",
       " 'sheaves',\n",
       " 'Gera',\n",
       " 'Thahash',\n",
       " 'seas',\n",
       " 'honourable',\n",
       " 'Jaalam',\n",
       " 'cometh',\n",
       " 'giants',\n",
       " 'abated',\n",
       " 'Tubal',\n",
       " 'gods',\n",
       " 'priests',\n",
       " 'troughs',\n",
       " 'Jegarsahadutha',\n",
       " 'Phallu',\n",
       " 'Zebul',\n",
       " 'presented',\n",
       " 'lights',\n",
       " 'grapes',\n",
       " 'Forgive',\n",
       " 'Bilhah',\n",
       " 'Dothan',\n",
       " 'Ehi',\n",
       " 'hil',\n",
       " 'Erech',\n",
       " 'hor',\n",
       " 'Salem',\n",
       " 'held',\n",
       " 'Ishmeelites',\n",
       " 'messes',\n",
       " 'began',\n",
       " 'sepulchres',\n",
       " 'Unstable',\n",
       " 'knees',\n",
       " 'Sabtech',\n",
       " 'Gerar',\n",
       " 'twel',\n",
       " 'Zillah',\n",
       " 'oversig',\n",
       " 'circumcis',\n",
       " 'Hittites',\n",
       " 'Melchizedek',\n",
       " 'Anamim',\n",
       " 'Ararat',\n",
       " 'proceedeth',\n",
       " 'awaked',\n",
       " 'fathers',\n",
       " 'Ludim',\n",
       " 'Hazezontamar',\n",
       " 'Arvadite',\n",
       " 'Edomites',\n",
       " 'possessi',\n",
       " 'denied',\n",
       " 'Gether',\n",
       " 'divineth',\n",
       " 'badest',\n",
       " 'Get',\n",
       " 'Mahalath',\n",
       " 'feet',\n",
       " 'interpretations',\n",
       " 'overthrew',\n",
       " 'burdens',\n",
       " 'fountains',\n",
       " 'fearest',\n",
       " 'Appoint',\n",
       " 'Merari',\n",
       " 'leaped',\n",
       " 'Zibeon',\n",
       " 'Jubal',\n",
       " 'twins',\n",
       " 'hadst',\n",
       " 'wiv',\n",
       " 'gifts',\n",
       " 'standest',\n",
       " 'trembled',\n",
       " 'colts',\n",
       " 'appeared',\n",
       " 'seasons',\n",
       " 'magnified',\n",
       " 'inhabitants',\n",
       " 'laughed',\n",
       " 'wandered',\n",
       " 'stronger',\n",
       " 'Becher',\n",
       " 'reigned',\n",
       " 'serva',\n",
       " 'firstlings',\n",
       " 'joined',\n",
       " 'findeth',\n",
       " 'If',\n",
       " 'Ezbon',\n",
       " 'longeth',\n",
       " 'Moreover',\n",
       " ',',\n",
       " 'fai',\n",
       " 'Haran',\n",
       " 'Cherubims',\n",
       " 'answered',\n",
       " 'Kenaz',\n",
       " 'Machpelah',\n",
       " 'Maachah',\n",
       " 'Potipherah',\n",
       " 'conspired',\n",
       " 'leanfleshed',\n",
       " 'loved',\n",
       " 'saidst',\n",
       " 'talked',\n",
       " 'caused',\n",
       " 'Remain',\n",
       " 'rested',\n",
       " 'Birsha',\n",
       " 'Our',\n",
       " 'tarried',\n",
       " 'Huppim',\n",
       " 'Assyria',\n",
       " 'Eshcol',\n",
       " 'Allonbachuth',\n",
       " 'Shinab',\n",
       " 'appointed',\n",
       " 'compassed',\n",
       " 'kids',\n",
       " 'Chesed',\n",
       " 'hands',\n",
       " 'Aholibamah',\n",
       " 'Ophir',\n",
       " 'Zarah',\n",
       " 'tongues',\n",
       " 'Thorns',\n",
       " 'Amraphel',\n",
       " 'created',\n",
       " 'Cause',\n",
       " 'shepherds',\n",
       " ';',\n",
       " 'hou',\n",
       " 'ewes',\n",
       " 'After',\n",
       " 'nuts',\n",
       " 'Reu',\n",
       " 'Kedemah',\n",
       " 'Hanoch',\n",
       " 'shouldest',\n",
       " 'Letushim',\n",
       " 'Diklah',\n",
       " 'Hadar',\n",
       " 'tribes',\n",
       " 'urged',\n",
       " 'Cush',\n",
       " 'Magdiel',\n",
       " 'Ashbel',\n",
       " 'defiledst',\n",
       " 'pris',\n",
       " 'months',\n",
       " 'Gomorrah',\n",
       " 'He',\n",
       " 'ste',\n",
       " 'Kadesh',\n",
       " 'ki',\n",
       " ',)',\n",
       " 'bak',\n",
       " 'Pinon',\n",
       " 'castles',\n",
       " 'verified',\n",
       " 'Succoth',\n",
       " 'colours',\n",
       " 'Dinhabah',\n",
       " 'songs',\n",
       " 'words',\n",
       " 'knoweth',\n",
       " 'Machir',\n",
       " 'Togarmah',\n",
       " 'failed',\n",
       " 'seest',\n",
       " 'hasted',\n",
       " 'Unto',\n",
       " 'Pison',\n",
       " 'concubines',\n",
       " 'Amalek',\n",
       " 'instruments',\n",
       " 'Horites',\n",
       " 'Amalekites',\n",
       " 'Iram',\n",
       " 'Horite',\n",
       " 'This',\n",
       " 'Eno',\n",
       " 'Kohath',\n",
       " 'redeemed',\n",
       " 'walketh',\n",
       " 'menservants',\n",
       " 'That',\n",
       " 'Zimran',\n",
       " 'Day',\n",
       " 'Belah',\n",
       " 'Midian',\n",
       " 'servan',\n",
       " 'decreased',\n",
       " 'Din',\n",
       " 'Hirah',\n",
       " 'commanded',\n",
       " 'years',\n",
       " 'sowed',\n",
       " 'supplanted',\n",
       " 'bracelets',\n",
       " 'womenservan',\n",
       " 'Epher',\n",
       " 'Eshban',\n",
       " 'putting',\n",
       " 'beasts',\n",
       " 'Lift',\n",
       " 'Ahuzzath',\n",
       " 'Asshur',\n",
       " 'Arioch',\n",
       " 'visions',\n",
       " 'Now',\n",
       " 'Chedorlaomer',\n",
       " 'bands',\n",
       " 'bakemeats',\n",
       " 'entreated',\n",
       " 'Hebron',\n",
       " 'Bozrah',\n",
       " 'numbering',\n",
       " 'Rebek',\n",
       " 'rebelled',\n",
       " 'shoulders',\n",
       " 'hang',\n",
       " 'westwa',\n",
       " 'Shaveh',\n",
       " 'delivered',\n",
       " 'daughters',\n",
       " 'camest',\n",
       " 'Kemuel',\n",
       " 'mayest',\n",
       " 'tents',\n",
       " 'Shall',\n",
       " 'Should',\n",
       " 'baskets',\n",
       " 'Issachar',\n",
       " 'laded',\n",
       " 'threshingfloor',\n",
       " 'spies',\n",
       " 'wor',\n",
       " 'Huz',\n",
       " 'nourished',\n",
       " 'Accad',\n",
       " 'hated',\n",
       " 'sojourned',\n",
       " 'Seir',\n",
       " 'stones',\n",
       " 'Nay',\n",
       " 'Galeed',\n",
       " 'bou',\n",
       " 'imagined',\n",
       " 'goeth',\n",
       " ')',\n",
       " 'Ishuah',\n",
       " 'gard',\n",
       " 'dost',\n",
       " 'women',\n",
       " 'Whoso',\n",
       " 'handmaids',\n",
       " 'stooped',\n",
       " 'Timnath',\n",
       " 'Moreh',\n",
       " 'spake',\n",
       " 'windows',\n",
       " 'died',\n",
       " 'Salah',\n",
       " 'hundredfo',\n",
       " 'meanest',\n",
       " 'hairs',\n",
       " 'liveth',\n",
       " 'In',\n",
       " 'princes',\n",
       " 'thi',\n",
       " 'shewed',\n",
       " 'refused',\n",
       " 'Abelmizraim',\n",
       " 'weapons',\n",
       " 'Avith',\n",
       " 'asswaged',\n",
       " 'Mibsam',\n",
       " 'Spirit',\n",
       " 'lentiles',\n",
       " 'Twelve',\n",
       " 'reviv',\n",
       " 'Hai',\n",
       " 'buryingplace',\n",
       " 'booths',\n",
       " 'Chezib',\n",
       " 'Moabites',\n",
       " 'Kenizzites',\n",
       " 'prospered',\n",
       " 'Hazarmaveth',\n",
       " 'Chaldees',\n",
       " 'Uzal',\n",
       " 'Dishan',\n",
       " 'Ebal',\n",
       " 'goats',\n",
       " 'Gentiles',\n",
       " 'budded',\n",
       " 'boys',\n",
       " 'erected',\n",
       " 'Japhe',\n",
       " 'giveth',\n",
       " 'Amorites',\n",
       " 'mules',\n",
       " 'forgat',\n",
       " 'Potiphar',\n",
       " 'thousands',\n",
       " 'Isui',\n",
       " 'Mehujael',\n",
       " 'Jachin',\n",
       " 'names',\n",
       " 'daughers',\n",
       " 'sto',\n",
       " 'Edom',\n",
       " 'Phichol',\n",
       " 'foals',\n",
       " 'slayeth',\n",
       " 'departing',\n",
       " 'espied',\n",
       " 'Whose',\n",
       " 'fathe',\n",
       " 'Hamul',\n",
       " 'Calneh',\n",
       " 'Zeboiim',\n",
       " 'With',\n",
       " 'waxed',\n",
       " 'worshipped',\n",
       " 'remained',\n",
       " 'Ashteroth',\n",
       " 'Philistines',\n",
       " 'heels',\n",
       " 'Havilah',\n",
       " 'obeyed',\n",
       " 'But',\n",
       " 'All',\n",
       " 'guiding',\n",
       " 'Abrah',\n",
       " 'sceptre',\n",
       " 'Haste',\n",
       " 'communed',\n",
       " 'gathered',\n",
       " 'Ask',\n",
       " 'Buz',\n",
       " 'vessels',\n",
       " 'En',\n",
       " 'ringstraked',\n",
       " 'entered',\n",
       " 'tim',\n",
       " 'Mahalaleel',\n",
       " 'See',\n",
       " 'favoured',\n",
       " 'Elah',\n",
       " 'prayed',\n",
       " 'places',\n",
       " 'followed',\n",
       " 'Terah',\n",
       " 'Wherefore',\n",
       " 'Zohar',\n",
       " 'Karnaim',\n",
       " 'Discern',\n",
       " 'speaketh',\n",
       " 'changed',\n",
       " 'Dumah',\n",
       " 'Mezahab',\n",
       " 'plains',\n",
       " 'Aran',\n",
       " 'Mamre',\n",
       " 'Hebrews',\n",
       " 'Ephra',\n",
       " 'Tamar',\n",
       " 'grisled',\n",
       " 'Hereby',\n",
       " 'intreated',\n",
       " 'Hast',\n",
       " 'Make',\n",
       " 'Heth',\n",
       " 'barr',\n",
       " 'communing',\n",
       " 'Hitti',\n",
       " 'Zidon',\n",
       " 'lads',\n",
       " 'Euphrates',\n",
       " 'Sheleph',\n",
       " 'EleloheIsrael',\n",
       " 'charged',\n",
       " 'suffered',\n",
       " 'Haggi',\n",
       " 'carcases',\n",
       " 'Take',\n",
       " 'Abide',\n",
       " 'served',\n",
       " 'rewarded',\n",
       " 'Jidlaph',\n",
       " 'compasseth',\n",
       " 'Naphtali',\n",
       " 'mocked',\n",
       " 'Egyptians',\n",
       " 'Abimelech',\n",
       " 'perceived',\n",
       " 'whosoever',\n",
       " 'togeth',\n",
       " 'coats',\n",
       " 'Eliphaz',\n",
       " 'parts',\n",
       " 'pleased',\n",
       " 'grisl',\n",
       " 'slimepits',\n",
       " 'fetcht',\n",
       " 'Let',\n",
       " 'garmen',\n",
       " 'findest',\n",
       " 'nati',\n",
       " 'Swear',\n",
       " 'Jezer',\n",
       " 'killed',\n",
       " 'journeys',\n",
       " 'Sitnah',\n",
       " 'shoelatchet',\n",
       " 'Both',\n",
       " 'weig',\n",
       " 'Peradventure',\n",
       " 'Serug',\n",
       " 'concubi',\n",
       " 'Keturah',\n",
       " 'fatfleshed',\n",
       " 'aprons',\n",
       " 'citi',\n",
       " 'waited',\n",
       " 'pitched',\n",
       " 'Ephrath',\n",
       " 'Assyr',\n",
       " 'rulers',\n",
       " 'Kittim',\n",
       " 'souls',\n",
       " 'youngest',\n",
       " 'officers',\n",
       " 'Zilpah',\n",
       " 'Sell',\n",
       " 'interpreted',\n",
       " 'Philistim',\n",
       " 'friends',\n",
       " 'sheepshearers',\n",
       " 'His',\n",
       " 'Zaphnathpaaneah',\n",
       " 'Peace',\n",
       " 'Shemeber',\n",
       " 'hearkened',\n",
       " 'Of',\n",
       " 'physicians',\n",
       " 'comforted',\n",
       " 'Drink',\n",
       " 'Ye',\n",
       " 'separated',\n",
       " 'catt',\n",
       " 'enemies',\n",
       " 'Joktan',\n",
       " 'Beriah',\n",
       " 'Benam',\n",
       " 'Phuvah',\n",
       " 'Egyptia',\n",
       " 'Sod',\n",
       " 'vestures',\n",
       " 'birds',\n",
       " 'Me',\n",
       " 'Mesha',\n",
       " 'borders',\n",
       " 'Anah',\n",
       " 'called',\n",
       " 'Dishon',\n",
       " 'bre',\n",
       " 'restored',\n",
       " 'needeth',\n",
       " 'prevailed',\n",
       " 'discerned',\n",
       " 'escaped',\n",
       " 'Naphish',\n",
       " 'Heaven',\n",
       " 'Canaanites',\n",
       " 'loins',\n",
       " 'Bera',\n",
       " 'mightier',\n",
       " 'dreams',\n",
       " 'lords',\n",
       " 'co',\n",
       " ';)',\n",
       " 'beari',\n",
       " ':',\n",
       " 'Sarai',\n",
       " 'Ajah',\n",
       " 'Asshurim',\n",
       " 'Come',\n",
       " 'Hazo',\n",
       " 'Peniel',\n",
       " 'So',\n",
       " 'Shepho',\n",
       " 'sheddeth',\n",
       " 'firstborn',\n",
       " 'Zebulun',\n",
       " 'shew',\n",
       " 'Jahleel',\n",
       " 'Only',\n",
       " 'bodies',\n",
       " 'hast',\n",
       " 'Ham',\n",
       " 'Except',\n",
       " 'devoured',\n",
       " 'households',\n",
       " 'changes',\n",
       " 'Shammah',\n",
       " 'Also',\n",
       " 'kine',\n",
       " 'Sichem',\n",
       " 'sepulchre',\n",
       " 'lifted',\n",
       " 'Happy',\n",
       " 'spe',\n",
       " 'kissed',\n",
       " 'ears',\n",
       " 'Slay',\n",
       " 'prisoners',\n",
       " 'Ur',\n",
       " 'marriages',\n",
       " 'camels',\n",
       " 'anointedst',\n",
       " 'Every',\n",
       " 'longedst',\n",
       " 'named',\n",
       " 'Beware',\n",
       " 'isles',\n",
       " 'bones',\n",
       " 'loveth',\n",
       " 'offeri',\n",
       " 'There',\n",
       " 'kings',\n",
       " 'Jeush',\n",
       " 'clo',\n",
       " 'dreamed',\n",
       " 'breasts',\n",
       " 'Sered',\n",
       " 'wells',\n",
       " 'seekest',\n",
       " 'Fill',\n",
       " 'toucheth',\n",
       " 'generatio',\n",
       " 'On',\n",
       " 'Samlah',\n",
       " 'Midianites',\n",
       " 'Hagar',\n",
       " 'cubits',\n",
       " 'bakers',\n",
       " 'journeyed',\n",
       " 'Why',\n",
       " 'Penuel',\n",
       " 'Earth',\n",
       " 'clothed',\n",
       " 'builded',\n",
       " 'measures',\n",
       " 'strangers',\n",
       " 'ou',\n",
       " 'Arbah',\n",
       " 'Fear',\n",
       " 'committed',\n",
       " 'repenteth',\n",
       " 'While',\n",
       " 'feebler',\n",
       " 'droves',\n",
       " 'bundles',\n",
       " '.',\n",
       " 'Emins',\n",
       " 'pieces',\n",
       " 'appe',\n",
       " 'selfwill',\n",
       " 'fema',\n",
       " 'repented',\n",
       " 'savoury',\n",
       " 'Return',\n",
       " 'And',\n",
       " 'Casluhim',\n",
       " 'looked',\n",
       " 'lieth',\n",
       " 'Carmi',\n",
       " 'Cheran',\n",
       " 'To',\n",
       " 'Look',\n",
       " 'arrayed',\n",
       " 'mandrakes',\n",
       " 'gavest',\n",
       " 'Bashemath',\n",
       " 'chariots',\n",
       " 'Ezer',\n",
       " 'Pildash',\n",
       " 'Rephaims',\n",
       " 'Put',\n",
       " 'attained',\n",
       " 'endued',\n",
       " 'plagues',\n",
       " 'Jebusites',\n",
       " 'commandments',\n",
       " 'Naphtuhim',\n",
       " 'Bilhan',\n",
       " 'Yet',\n",
       " 'skins',\n",
       " 'Lie',\n",
       " 'When',\n",
       " 'Padan',\n",
       " 'Set',\n",
       " 'strakes',\n",
       " 'Moab',\n",
       " 'ships',\n",
       " 'Kadmonites',\n",
       " 'tru',\n",
       " 'seemed',\n",
       " 'Stand',\n",
       " 'required',\n",
       " 'Hori',\n",
       " 'childr',\n",
       " 'It',\n",
       " 'Kenites',\n",
       " 'Padanaram',\n",
       " 'By',\n",
       " 'Masrekah',\n",
       " 'embalmed',\n",
       " 'Obal',\n",
       " 'numbered',\n",
       " 'Thy',\n",
       " 'Ard',\n",
       " 'refrained',\n",
       " 'increased',\n",
       " 'Know',\n",
       " 'families',\n",
       " 'Shed',\n",
       " 'lands',\n",
       " 'believed',\n",
       " 'Tebah',\n",
       " 'fishes',\n",
       " 'Binding',\n",
       " 'drinketh',\n",
       " 'Some',\n",
       " 'Surely',\n",
       " 'Whence',\n",
       " 'Be',\n",
       " 'Luz',\n",
       " 'Mesopotamia',\n",
       " 'thistles',\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We saw it is working for small set above. Let's run it on text3 from nltk.book over all corpus words from nltk package:\n",
    "\n",
    "text_vs_vocabulary(text3, nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Import the itemgetter() function from the operator module in Python's standard library (i.e. from operator import itemgetter). Create a list words containing several words. For example, words= ['The', 'dog', 'gave', 'John', 'the', 'newspaper']. Now try calling: sorted(words, key=itemgetter(1)), and sorted(words, key=itemgetter(-1)). Explain what itemgetter() is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gave', 'newspaper', 'The', 'the', 'dog', 'John']\n",
      "['The', 'gave', 'the', 'dog', 'John', 'newspaper']\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Given example:\n",
    "\n",
    "words= ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "print(sorted(words, key=itemgetter(1)))\n",
    "print(sorted(words, key=itemgetter(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['schools', 'New York', 'returning', 'week', 'the', 'this', 'classroom', 'in', 'to', 'are', 'Public']\n",
      "['Public', 'are', 'the', 'returning', 'New York', 'week', 'classroom', 'in', 'to', 'schools', 'this']\n"
     ]
    }
   ],
   "source": [
    "# Another example:\n",
    "\n",
    "new_words= ['Public', 'schools', 'in', 'New York', 'are', 'returning', 'to', 'the', 'classroom', 'this', 'week']\n",
    "print(sorted(new_words, key=itemgetter(1)))\n",
    "print(sorted(new_words, key=itemgetter(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class itemgetter in module operator:\n",
      "\n",
      "class itemgetter(builtins.object)\n",
      " |  itemgetter(item, ...) --> itemgetter object\n",
      " |  \n",
      " |  Return a callable object that fetches the given item(s) from its operand.\n",
      " |  After f = itemgetter(2), the call f(r) returns r[2].\n",
      " |  After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, /, *args, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Return state information for pickling\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(itemgetter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explanation: Operator is a built-in module providing a set of convenient operators -- operator.itemgetter(n) constructs a callable that assumes an iterable object (e.g. list, tuple, set) as input, and fetches the n-th element out of it. I have called the help manual above to read the functional details from the developer of the package.***\n",
    "\n",
    "***Therefore in the given example itemgetter(1) reads value in alphabetical order of the item in 1st index while itemgetter(-1) does it based on the last index.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
