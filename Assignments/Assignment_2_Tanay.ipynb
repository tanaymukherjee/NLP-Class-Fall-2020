{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Tanay Mukherjee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Describe the class of strings matched by the following regular expressions.No code is needed and just describe what the following regular expressions do/match).\n",
    "\n",
    "#a.[a-zA-Z]+\n",
    "#b.[A-Z][a-z]*\n",
    "#c.p[aeiou]{,2}t\n",
    "#d.\\d+(\\.\\d+)?\n",
    "#e.([^aeiou][aeiou][^aeiou])*\n",
    "#f.\\w+|[^\\w\\s]+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{The} {companies}, {Moderna} {and} {Pfizer}, {revealed} {details} {about} {how} {participants} {are} {being} {selected} {and} {monitored}, {the} {conditions} {under} {which} {the} {trials} {could} {be} {stopped} {early} {if} {there} {were} {problems}, {and} {the} {evidence} {researchers} {will} {use} {to} {determine} {whether} {people} {who} {got} {the} {vaccines} {were} {protected} {from} {Covid}-19.\n"
     ]
    }
   ],
   "source": [
    "test = \"The companies, Moderna and Pfizer, revealed details about how participants are being selected and monitored, the conditions under which the trials could be stopped early if there were problems, and the evidence researchers will use to determine whether people who got the vaccines were protected from Covid-19.\"\n",
    "# Took the test data from NY Times blog\n",
    "\n",
    "# a.\n",
    "nltk.re_show(r'[a-zA-Z]+', test)\n",
    "\n",
    "# Ans: It is matching anything that has alphabets, whether or not it has upper case or lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{The} companies, {Moderna} and {Pfizer}, revealed details about how participants are being selected and monitored, the conditions under which the trials could be stopped early if there were problems, and the evidence researchers will use to determine whether people who got the vaccines were protected from {Covid}-19.\n"
     ]
    }
   ],
   "source": [
    "# b.\n",
    "nltk.re_show(r'[A-Z][a-z]*', test)\n",
    "\n",
    "# Ans: All capitalized words are matched which has first letter as capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The companies, Moderna and Pfizer, revealed details about how participants are being selected and monitored, the conditions under which the trials could be stopped early if there were problems, and the evidence researchers will use to determine whether people who got the vaccines were protected from Covid-19.\n",
      "pant\n",
      "{pat}\n",
      "{pout}\n",
      "pan\n",
      "{pt}\n"
     ]
    }
   ],
   "source": [
    "# c.\n",
    "nltk.re_show(r'p[aeiou]{,2}t', test)\n",
    "\n",
    "# Ans: Words starting with p and ending in t. And between them either 0, 1 or 2 vowels from english language.\n",
    "\n",
    "# The example I chose had not like that so see a new example below:\n",
    "nltk.re_show(r'p[aeiou]{,2}t', \"pant\")\n",
    "nltk.re_show(r'p[aeiou]{,2}t', \"pat\")\n",
    "nltk.re_show(r'p[aeiou]{,2}t', \"pout\")\n",
    "nltk.re_show(r'p[aeiou]{,2}t', \"pan\")\n",
    "nltk.re_show(r'p[aeiou]{,2}t', \"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The companies, Moderna and Pfizer, revealed details about how participants are being selected and monitored, the conditions under which the trials could be stopped early if there were problems, and the evidence researchers will use to determine whether people who got the vaccines were protected from Covid-{19}.\n"
     ]
    }
   ],
   "source": [
    "# d.\n",
    "nltk.re_show(r'\\d+(\\.\\d+)?', test)\n",
    "\n",
    "# Ans: It will return any number - whole, fraction or integer. Anything that falls in the number line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}T{he compan}{}i{}e{}s{},{} {Mod}{}e{}r{na }{}a{}n{}d{} {}P{fiz}{}e{}r{},{} {rev}{}e{}a{led}{} {det}{}a{}i{}l{}s{ ab}{}o{}u{}t{} {how}{} {partic}{}i{pan}{}t{}s{ ar}{}e{} {}b{}e{}i{}n{}g{} {sel}{}e{}c{ted an}{}d{} {mon}{}i{tor}{}e{}d{},{} {}t{he condit}{}i{}o{}n{}s{ under}{} {}w{hic}{}h{} {}t{he }{}t{}r{}i{}a{}l{}s{} {}c{}o{}u{}l{}d{} {be }{}s{topped}{} {}e{}a{}r{}l{}y{ if}{} {}t{her}{}e{} {wer}{}e{} {}p{roblem}{}s{},{ an}{}d{} {}t{he }{}e{vid}{}e{}n{ce res}{}e{}a{}r{}c{her}{}s{} {wil}{}l{ us}{}e{} {to det}{}e{}r{min}{}e{} {}w{hether}{} {}p{}e{}o{}p{le }{}w{ho got}{} {}t{he vaccin}{}e{}s{} {wer}{}e{} {}p{rot}{}e{}c{ted}{} {}f{rom}{} {Cov}{}i{}d{}-{}1{}9{}.{}\n"
     ]
    }
   ],
   "source": [
    "# e.\n",
    "nltk.re_show(r'([^aeiou][aeiou][^aeiou])*', test)\n",
    "\n",
    "# Ans: Zero or more sequence of a combination of consonants-vowel-consonants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{The} {companies}{,} {Moderna} {and} {Pfizer}{,} {revealed} {details} {about} {how} {participants} {are} {being} {selected} {and} {monitored}{,} {the} {conditions} {under} {which} {the} {trials} {could} {be} {stopped} {early} {if} {there} {were} {problems}{,} {and} {the} {evidence} {researchers} {will} {use} {to} {determine} {whether} {people} {who} {got} {the} {vaccines} {were} {protected} {from} {Covid}{-}{19}{.}\n"
     ]
    }
   ],
   "source": [
    "# f.\n",
    "nltk.re_show(r'\\w+|[^\\w\\s]+', test)\n",
    "\n",
    "# Ans: All alphanumeric characters including punctuation and non-whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Rewrite the following loop as a list comprehension:\n",
    "# sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "# result = []\n",
    "# for word in sent:\n",
    "#     word_len = (word, len(word))\n",
    "#     result.append(word_len)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the sample code:\n",
    "\n",
    "sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Re-writing it as a list comprehension:\n",
    "\n",
    "sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "print([(w, len(w)) for w in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Read in some text from your own document in your local disk, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"The companies, Moderna and Pfizer, revealed details about how participants are being selected and monitored, the conditions under which the trials could be stopped early if there were problems, and the evidence researchers will use to determine whether people who got the vaccines were protected from Covid-19.\"\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The text in the file is same as what I used above for regex check\n",
    "\n",
    "'''\n",
    "\"The companies, Moderna and Pfizer, revealed details about how participants are being selected and monitored, the conditions under which the trials could be stopped early if there were problems, and the evidence researchers will use to determine whether people who got the vaccines were protected from Covid-19.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='test.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = file.read()\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['which', 'whether', 'who']\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Using the method startswith()\n",
    "\n",
    "print([wh for wh in tokens if wh.lower().startswith('wh')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['which', 'whether', 'who']\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Using regex expression\n",
    "\n",
    "print([wh for wh in tokens if re.findall('^[Ww]h', wh)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create your own file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhello 5\\ntanay 5\\nnlp 3\\ngoodbye 7\\nfriday 6\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Content of the file que4.txt is as follows:\n",
    "\n",
    "'''\n",
    "hello 5\n",
    "tanay 5\n",
    "nlp 3\n",
    "goodbye 7\n",
    "friday 6\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', 5], ['tanay', 5], ['nlp', 3], ['goodbye', 7], ['friday', 6]]\n"
     ]
    }
   ],
   "source": [
    "que4 = open('que4.txt', encoding = \"utf-8\").readlines()\n",
    "print([[w, int(i)] for w, i in (rows.split() for rows in que4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oje's doubt:\n",
    "\n",
    "myfile2= open('que4_v2.txt','r')\n",
    "raw1 = myfile2.read()\n",
    "fields = raw1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 5], ['tanay', 5], ['nlp', 3], ['goodbye', 7], ['friday', 6]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed Oje's code:\n",
    "\n",
    "result1 = [] # Creates an empty list called result1\n",
    "x=0 # Initiate with a dummy index\n",
    "for i in fields: # this function checks fields and appends results1 with 'strings' and changes numbers to integer\n",
    "    if i.isalpha():\n",
    "        result1.append([i])\n",
    "    else:\n",
    "        result1[x].append(int(i))\n",
    "        x = x+1\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello 5', 'tanay 5', 'nlp 3', 'goodbye 7', 'friday 6']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alyssa's Code:\n",
    "\n",
    "[s.strip() for s in que4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for each section of the Brown Corpus (i.e. News, Editorial,… Humor) (Hint: for category in brown.categories( )). Make use of the fact that nltk.corpus.brown.words() produces a sequence of words, while nltk.corpus.brown.sents() produces a sequence of sentences. (Hint: from nltk.corpus import brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARI(category):\n",
    "    words = brown.words(categories = category)\n",
    "    sents = brown.sents(categories = category)\n",
    "    μw = sum(len(w) for w in words)/len(words)\n",
    "    μs = sum(len(s) for s in sents)/len(sents)\n",
    "    return (4.71 * μw + 0.5 * μs - 21.43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The category is adventure       and its corresponding ARI is 4.08\n",
      "The category is belles_lettres  and its corresponding ARI is 10.99\n",
      "The category is editorial       and its corresponding ARI is 9.47\n",
      "The category is fiction         and its corresponding ARI is 4.91\n",
      "The category is government      and its corresponding ARI is 12.08\n",
      "The category is hobbies         and its corresponding ARI is 8.92\n",
      "The category is humor           and its corresponding ARI is 7.89\n",
      "The category is learned         and its corresponding ARI is 11.93\n",
      "The category is lore            and its corresponding ARI is 10.25\n",
      "The category is mystery         and its corresponding ARI is 3.83\n",
      "The category is news            and its corresponding ARI is 10.18\n",
      "The category is religion        and its corresponding ARI is 10.2\n",
      "The category is reviews         and its corresponding ARI is 10.77\n",
      "The category is romance         and its corresponding ARI is 4.35\n",
      "The category is science_fiction and its corresponding ARI is 4.98\n"
     ]
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    print(\"The category is {0:<15} and its corresponding ARI is {1}\". format(category, round(ARI(category),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Use the Porter Stemmer to normalize some tokenized text (see below), calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and describe any difference you observe by using these two stemmers.\n",
    "# text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    " text = 'Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technolog', 'base', 'on', 'nlp', 'are', 'becom', 'increasingli', 'widespread', '.', 'for', 'exampl', ',', 'phone', 'and', 'handheld', 'comput', 'support', 'predict', 'text', 'and', 'handwrit', 'recognit', ';', 'web', 'search', 'engin', 'give', 'access', 'to', 'inform', 'lock', 'up', 'in', 'unstructur', 'text', ';', 'machin', 'translat', 'allow', 'us', 'to', 'retriev', 'text', 'written', 'in', 'chines', 'and', 'read', 'them', 'in', 'spanish', ';', 'text', 'analysi', 'enabl', 'us', 'to', 'detect', 'sentiment', 'in', 'tweet', 'and', 'blog', '.', 'By', 'provid', 'more', 'natur', 'human-machin', 'interfac', ',', 'and', 'more', 'sophist', 'access', 'to', 'store', 'inform', ',', 'languag', 'process', 'ha', 'come', 'to', 'play', 'a', 'central', 'role', 'in', 'the', 'multilingu', 'inform', 'societi']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technolog', 'bas', 'on', 'nlp', 'ar', 'becom', 'increas', 'widespread', '.', 'for', 'exampl', ',', 'phon', 'and', 'handheld', 'comput', 'support', 'predict', 'text', 'and', 'handwrit', 'recognit', ';', 'web', 'search', 'engin', 'giv', 'access', 'to', 'inform', 'lock', 'up', 'in', 'unstruct', 'text', ';', 'machin', 'transl', 'allow', 'us', 'to', 'retriev', 'text', 'writ', 'in', 'chines', 'and', 'read', 'them', 'in', 'span', ';', 'text', 'analys', 'en', 'us', 'to', 'detect', 'senty', 'in', 'tweet', 'and', 'blog', '.', 'by', 'provid', 'mor', 'nat', 'human-machine', 'interfac', ',', 'and', 'mor', 'soph', 'access', 'to', 'stor', 'inform', ',', 'langu', 'process', 'has', 'com', 'to', 'play', 'a', 'cent', 'rol', 'in', 'the', 'multil', 'inform', 'socy']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:** Porter algorithm is the less aggressive as an algorithm. The stems of the words are somewhat intuitive and are understandable. On the other hand, Lancaster algorithm is very aggressive because of its strictly chopping words and making it much confusing. With this algorithm in use, the stems become non-relatable to some extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. Please compare the reading difficulties for ABC Rural News (\"rural.txt\") and ABC Science News(\"science.txt\") (nltk.corpus.abc).(Hint: from nltk.corpus import abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import abc\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC Rural News has an ARI of   12.62\n",
      "ABC Science News has an ARI of 12.77\n"
     ]
    }
   ],
   "source": [
    "def ARI(category):\n",
    "    words = word_tokenize(category)\n",
    "    sents = [nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(category)]\n",
    "    μw = sum(len(w) for w in words)/len(words)\n",
    "    μs = sum(len(s) for s in sents)/len(sents)\n",
    "    return 4.71 * μw + 0.5 * μs - 21.43\n",
    "\n",
    "print(\"ABC Rural News has an ARI of {0:>7}\". format(round(ARI(abc.raw(\"rural.txt\")),2)))\n",
    "print(\"ABC Science News has an ARI of {0:>5}\". format(round(ARI(abc.raw(\"science.txt\")),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.Rewrite the following nested loop as a nested list comprehension:\n",
    "#  words = ['attribution', 'confabulation', 'elocution',\n",
    "#         'sequoia', 'tenacious', 'unidirectional']\n",
    "#  vsequences = set()\n",
    "#  for word in words:\n",
    "#      vowels = []\n",
    "#      for char in word:\n",
    "#          if char in 'aeiou':\n",
    "#              vowels.append(char)\n",
    "#      vsequences.add(''.join(vowels))\n",
    "#  sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "    vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-writing it as a list comprehension:\n",
    "# Method 1: Using a generic for loop for vowels and then on words array\n",
    "\n",
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "sorted(set([''.join([w for w in word if w in 'aeiou']) for word in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-writing it as a list comprehension:\n",
    "# Method 2: Using regex to identify vowels and then a for loop on words array\n",
    "\n",
    "sorted(re.sub('[^aeiou]', '', w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe Tragedie of Hamlet was written by William Shakespeare in 1599\\nLeaves of Grass        was written by Walt Whiteman       in 1855\\nEmma                   was written by Jane Austen         in 1816\\n# sample code:\\ntemplate = 'Lee wants a {} right now'\\nmenu = ['sandwich', 'spam fritter', 'pancake']\\nfor snack in menu:\\n    print(template.format(snack))\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.Try to refer the following sample code to print the following sentences in a formatted way.(Hint: you should use str.format() method in print() and for loop；For more information, please read the textbook section 3.9 in chapter 3) \n",
    "# output should look like:\n",
    "'''\n",
    "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
    "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
    "Emma                   was written by Jane Austen         in 1816\n",
    "# sample code:\n",
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich     right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake      right now\n"
     ]
    }
   ],
   "source": [
    "# How will it work for the given sample:\n",
    "\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for i in range (len (menu)):\n",
    "    print(\"{0:<11} {1:<12} {2}\".format('Lee wants a', menu[i], 'right now'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
      "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
      "Emma                   was written by Jane Austen         in 1816\n"
     ]
    }
   ],
   "source": [
    "# How will it work for the given question:\n",
    "\n",
    "books = ['The Tragedie of Hamlet', 'Leaves of Grass', 'Emma']\n",
    "author = ['William Shakespeare', 'Walt Whiteman', 'Jane Austen']\n",
    "year = [1599, 1855, 1816]\n",
    "for i in range (len (books)):\n",
    "    print(\"{0:<22} was written by {1:<19} in {2}\".format(books[i], author[i], year[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Define the variable quote to contain the list ['Action', 'speaks', 'louder', 'than', 'words']. Process this list using a for loop, and store the length of each word in a new list lengths. Hint: begin by assigning the empty list to lengths, using lengths = []. Then each time through the loop, use append() to add another length value to the list. Then do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = ['Action', 'speaks', 'louder', 'than', 'words']\n",
    "lengths = [[len(i) for i in x.split()] for x in quote]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6], [6], [6], [4], [5]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the word Action is [6]\n",
      "Length of the word speaks is [6]\n",
      "Length of the word louder is [6]\n",
      "Length of the word than   is [4]\n",
      "Length of the word words  is [5]\n"
     ]
    }
   ],
   "source": [
    "# Also printing the arrays: quote and lengths using string formatting\n",
    "\n",
    "for i in range (len (quote)):\n",
    "    print(\"Length of the word {0:<6} is {1}\".format(quote[i], lengths[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
